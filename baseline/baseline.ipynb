{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0420b38a-0de5-4db3-8631-eb28eace6db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 20:02:09.486786: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-23 20:02:09.495982: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-23 20:02:09.507320: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-23 20:02:09.510457: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-23 20:02:09.518665: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-23 20:02:10.081499: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "import albumentations as A\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.datasets as datasets\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import v2\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a652999e-000a-44d8-9a0c-959008026a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55265, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>file_path</th>\n",
       "      <th>modification</th>\n",
       "      <th>gender</th>\n",
       "      <th>hand</th>\n",
       "      <th>finger</th>\n",
       "      <th>method</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>./archive/SOCOFing/Real/1__M_Left_thumb_finger...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>./archive/SOCOFing/Real/1__M_Right_ring_finger...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>./archive/SOCOFing/Real/1__M_Left_middle_finge...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>./archive/SOCOFing/Real/1__M_Right_index_finge...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>./archive/SOCOFing/Real/1__M_Left_index_finger...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55260</th>\n",
       "      <td>55260</td>\n",
       "      <td>55260</td>\n",
       "      <td>./archive/SOCOFing/Altered/Altered-Hard/600__M...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55261</th>\n",
       "      <td>55261</td>\n",
       "      <td>55261</td>\n",
       "      <td>./archive/SOCOFing/Altered/Altered-Hard/600__M...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55262</th>\n",
       "      <td>55262</td>\n",
       "      <td>55262</td>\n",
       "      <td>./archive/SOCOFing/Altered/Altered-Hard/600__M...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55263</th>\n",
       "      <td>55263</td>\n",
       "      <td>55263</td>\n",
       "      <td>./archive/SOCOFing/Altered/Altered-Hard/600__M...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55264</th>\n",
       "      <td>55264</td>\n",
       "      <td>55264</td>\n",
       "      <td>./archive/SOCOFing/Altered/Altered-Hard/600__M...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55265 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0.1  Unnamed: 0  \\\n",
       "0                 0           0   \n",
       "1                 1           1   \n",
       "2                 2           2   \n",
       "3                 3           3   \n",
       "4                 4           4   \n",
       "...             ...         ...   \n",
       "55260         55260       55260   \n",
       "55261         55261       55261   \n",
       "55262         55262       55262   \n",
       "55263         55263       55263   \n",
       "55264         55264       55264   \n",
       "\n",
       "                                               file_path  modification  \\\n",
       "0      ./archive/SOCOFing/Real/1__M_Left_thumb_finger...             0   \n",
       "1      ./archive/SOCOFing/Real/1__M_Right_ring_finger...             0   \n",
       "2      ./archive/SOCOFing/Real/1__M_Left_middle_finge...             0   \n",
       "3      ./archive/SOCOFing/Real/1__M_Right_index_finge...             0   \n",
       "4      ./archive/SOCOFing/Real/1__M_Left_index_finger...             0   \n",
       "...                                                  ...           ...   \n",
       "55260  ./archive/SOCOFing/Altered/Altered-Hard/600__M...             3   \n",
       "55261  ./archive/SOCOFing/Altered/Altered-Hard/600__M...             3   \n",
       "55262  ./archive/SOCOFing/Altered/Altered-Hard/600__M...             3   \n",
       "55263  ./archive/SOCOFing/Altered/Altered-Hard/600__M...             3   \n",
       "55264  ./archive/SOCOFing/Altered/Altered-Hard/600__M...             3   \n",
       "\n",
       "       gender  hand  finger  method   id  \n",
       "0           0     1       0       0    1  \n",
       "1           0     0       3       0    1  \n",
       "2           0     1       2       0    1  \n",
       "3           0     0       1       0    1  \n",
       "4           0     1       1       0    1  \n",
       "...       ...   ...     ...     ...  ...  \n",
       "55260       0     0       3       1  600  \n",
       "55261       0     0       4       2  600  \n",
       "55262       0     0       4       1  600  \n",
       "55263       0     0       3       2  600  \n",
       "55264       0     1       3       1  600  \n",
       "\n",
       "[55265 rows x 9 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('./fingerprint_data')\n",
    "print(df.shape)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dca4b948-1af1-402f-8716-a543f45e6f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=df[df['id']<500]\n",
    "val=df[df['id'].between(500,550)]\n",
    "test=df[df['id']>550]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0f44087-273f-48a2-9d4e-2f996d3de81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a simple residual block\n",
    "class SimpleResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(SimpleResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # If in_channels != out_channels or stride != 1, adjust the residual connection\n",
    "        self.adjust_residual = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride)\n",
    "        self.use_adjust = (in_channels != out_channels or stride != 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        \n",
    "        # Adjust the residual if necessary\n",
    "        if self.use_adjust:\n",
    "            residual = self.adjust_residual(x)\n",
    "        \n",
    "        out += residual\n",
    "        return F.relu(out)\n",
    "\n",
    "# Define a smaller ResNet-like model with 3 residual blocks\n",
    "class SmallResNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SmallResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        \n",
    "        # Define 3 residual blocks\n",
    "        self.block1 = SimpleResidualBlock(16, 32, stride=2)\n",
    "        self.block2 = SimpleResidualBlock(32, 64, stride=2)\n",
    "        self.block3 = SimpleResidualBlock(64, 128, stride=2)\n",
    "        \n",
    "        # Global average pooling and a fully connected layer\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.global_pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return self.fc(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e23e4b54-b0f3-4d82-830d-a0f4d8613a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class train_dataloader(torch.utils.data.Dataset):\n",
    "    def __init__(self, frame):\n",
    "        self.frame=frame\n",
    "        self.front_path = [f for f in frame['file_path'].tolist()]\n",
    "        self.pred = [a for a in frame['hand'].tolist()]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_conv=img_trf_train(str(self.front_path[index]))\n",
    "        image, target = img_conv, int(self.pred[index])\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frame)\n",
    "\n",
    "class val_dataloader(torch.utils.data.Dataset):\n",
    "    def __init__(self, frame):\n",
    "        self.frame=frame\n",
    "        self.front_path = [f for f in frame['file_path'].tolist()]\n",
    "        self.pred = [a for a in frame['hand'].tolist()]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_conv=img_trf_val(str(self.front_path[index]))\n",
    "        image, target = img_conv, int(self.pred[index])\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e4016c7-c008-4b72-8be7-c13551f461cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_trf_train(code):\n",
    "    try:\n",
    "        img=cv2.imread(str(code))#'../datasets/prisoner/side_crop/'+str(code)+'.jpg')#frissiteni kell\n",
    "        #max_side=max(img.shape[0],img.shape[1])\n",
    "        #print(max_side)\n",
    "        #add transform\n",
    "        transform=A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.Affine(scale=(0.9,1.1), shear=(-5,5), rotate=(-10,10), p=0.3,fit_output=False,keep_ratio=True),\n",
    "        A.GaussianBlur(blur_limit=(3, 7), sigma_limit=0, p=0.2),\n",
    "        A.GridDistortion (num_steps=5, distort_limit=(-0.3, 0.3), interpolation=1, border_mode=2, p=0.2),\n",
    "        A.RandomBrightnessContrast(p=0.4),\n",
    "        A.Resize(112,112),\n",
    "        ToTensorV2()])\n",
    "        front_transformed=transform(image=img)[\"image\"]\n",
    "        return front_transformed \n",
    "        \n",
    "    except:\n",
    "        z=torch.zeros(3,112,112)\n",
    "    return z\n",
    "\n",
    "def img_trf_val(code):\n",
    "    try:\n",
    "        img=cv2.imread(str(code))#'../datasets/prisoner/side_crop/'+str(code)+'.jpg')#frissiteni kell\n",
    "        #max_side=max(img.shape[0],img.shape[1])\n",
    "        #print(max_side)\n",
    "        #add transform\n",
    "        transform=A.Compose([\n",
    "        A.Resize(112,112),\n",
    "        ToTensorV2()])\n",
    "        front_transformed=transform(image=img)[\"image\"]\n",
    "        return front_transformed \n",
    "        \n",
    "    except:\n",
    "        z=torch.zeros(3,112,112)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c88ce06-36d6-4e27-9cc5-341f2fb52ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataloader(train)\n",
    "val_dataset=val_dataloader(val)\n",
    "test_dataset = val_dataloader(test)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "val_dataloader= DataLoader(val_dataset, batch_size=1024, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1024, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5395cb6-ac1f-4b34-b1ac-abfd5980f477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "813e30c9-886c-49b1-97f2-b7e5e5525373",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import gc\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "def train_model():\n",
    "    since = time.time()\n",
    "    best_error = 100\n",
    "    best_acc=0\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    train_acc_history = []\n",
    "    val_acc_history = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        running_corrects = 0\n",
    "        total_train_samples = 0\n",
    "\n",
    "        for images, masks in train_dataloader:\n",
    "            images = images.to(device=device, dtype=torch.float)\n",
    "            masks = masks.to(device=device, dtype=torch.long) \n",
    "            \n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)  # Get predictions\n",
    "            \n",
    "            loss = loss_function(outputs, masks)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            running_corrects += (preds == masks).sum().item()  # Sum up correct predictions\n",
    "            total_train_samples += masks.size(0)  # Count total samples in the batch\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        epoch_train_loss = running_loss / len(train_dataloader)\n",
    "        epoch_train_acc = running_corrects / total_train_samples\n",
    "        train_acc_history.append(epoch_train_acc)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        validation_loss = 0\n",
    "        validation_corrects = 0\n",
    "        total_val_samples = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_dataloader:\n",
    "                images = images.to(device=device, dtype=torch.float)\n",
    "                labels = labels.to(device=device, dtype=torch.long) \n",
    "\n",
    "                output_val = model(images)\n",
    "                val_loss_batch = loss_function(output_val, labels)\n",
    "                validation_loss += val_loss_batch.item()\n",
    "\n",
    "                _, val_preds = torch.max(output_val, 1)  # Get validation predictions\n",
    "                validation_corrects += (val_preds == labels).sum().item()  # Sum up correct predictions\n",
    "                total_val_samples += labels.size(0)  # Count total samples in the batch\n",
    "\n",
    "        epoch_val_loss = validation_loss / len(val_dataloader)\n",
    "        epoch_val_acc = validation_corrects / total_val_samples\n",
    "        val_acc_history.append(epoch_val_acc)\n",
    "\n",
    "        # Save best model based on validation loss\n",
    "        if epoch_val_acc >= best_acc:\n",
    "            best_acc=epoch_val_acc\n",
    "            #best_error = epoch_val_loss\n",
    "            best_epoch = epoch\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            torch.save(best_model, './hand_predict_base.pt')\n",
    "            print(f\"Best Model Saved - Validation Accuracy: {best_acc} at Epoch: {best_epoch+1}\")\n",
    "\n",
    "        # Print epoch metrics\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "              f\"Training Loss: {epoch_train_loss:.4f}, \"\n",
    "              f\"Training Accuracy: {epoch_train_acc * 100:.2f}%, \"\n",
    "              f\"Validation Loss: {epoch_val_loss:.4f}, \"\n",
    "              f\"Validation Accuracy: {epoch_val_acc * 100:.2f}%\")\n",
    "\n",
    "        # Clean up\n",
    "        gc.collect()\n",
    "        with torch.no_grad():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f\"Training time was: {time_elapsed:.2f} seconds\")\n",
    "    print(f\"Best Validation Accuracy: {best_acc} at Epoch: {best_epoch+1}\")\n",
    "\n",
    "    #return train_loss, val_loss, train_acc_history, val_acc_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "446f7ca8-eaa3-4f25-92d9-b9cebdf61eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = SmallResNet(num_classes=10)\n",
    "model.to(device)\n",
    "loss_function = nn.CrossEntropyLoss() \n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "#print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b017fb53-6e36-4d4c-8986-e03e569b1ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model Saved - Validation Accuracy: 0.5631668800682885 at Epoch: 1\n",
      "Epoch [1/20], Training Loss: 0.6883, Training Accuracy: 61.22%, Validation Loss: 1.1666, Validation Accuracy: 56.32%\n",
      "Best Model Saved - Validation Accuracy: 0.6717883055911225 at Epoch: 2\n",
      "Epoch [2/20], Training Loss: 0.6101, Training Accuracy: 66.86%, Validation Loss: 0.6386, Validation Accuracy: 67.18%\n"
     ]
    }
   ],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3c566d-cbe8-485c-89a6-0bf458790ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test= SmallResNet(num_classes=2)\n",
    "loss_function = nn.CrossEntropyLoss()  # Use CrossEntropyLoss for segmentation\n",
    "model_test.load_state_dict(torch.load('./hand_predict_base.pt',weights_only=True))\n",
    "model_test.to(device)\n",
    "model_test.eval()\n",
    "print('Model is loadad for testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79013be0-8a80-4605-b705-d824f1eac74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test.eval()\n",
    "validation_loss = 0\n",
    "validation_corrects = 0\n",
    "total_val_samples = 0\n",
    "val_acc_history=[]\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_dataloader:\n",
    "        images = images.to(device=device, dtype=torch.float) \n",
    "        labels = labels.to(device=device, dtype=torch.long) \n",
    "\n",
    "        output_val = model_test(images)\n",
    "        val_loss_batch = loss_function(output_val, labels)\n",
    "        validation_loss += val_loss_batch.item()\n",
    "\n",
    "        _, val_preds = torch.max(output_val, 1)  # Get validation predictions\n",
    "        validation_corrects += (val_preds == labels).sum().item()  # Sum up correct predictions\n",
    "        total_val_samples += labels.size(0)  # Count total samples in the batch\n",
    "\n",
    "epoch_val_loss = validation_loss / len(val_dataloader)\n",
    "epoch_val_acc = validation_corrects / total_val_samples\n",
    "val_acc_history.append(epoch_val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7505b32f-05a0-4a8d-967e-61b039648c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Test Accuracy: {epoch_val_acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f598053f-15cf-485f-bdf0-b730a46e6cac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bccb908-6bbc-41b0-ae4f-40426d8b6a3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
